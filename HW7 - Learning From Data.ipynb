{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following problems, use the data provided in the files in.dta and out.dta for Homework # 6. We are going to apply linear regression with a nonlinear transformation for classification (without regularization). The nonlinear transformation is given by $\\phi_0$ through $\\phi_7$ which transform ($x_1$, $x_2$) into\n",
    "\n",
    "$$1 \\quad x_1 \\quad x_2 \\quad x_1^2 \\quad x_2^2 \\quad x_1x_2 \\quad \\left |x_1 - x_2 \\right | \\quad \\left |x_1 + x_2 \\right | $$\n",
    "\n",
    "To illustrate how taking out points for validation affects the performance, we will consider the hypotheses trained on $\\mathcal{D}_{\\text{train}}$ (without restoring the full $\\mathcal{D}$ for training after validation is done).\n",
    "\n",
    "Q1. Split in.dta into training (first 25 examples) and validation (last 10 examples). Train on the 25 examples only, using the validation set of 10 examples to select between five models that apply linear regression to $\\phi_0$ through $\\phi_k$, with $k = 3, 4, 5, 6, 7$. For which model is the classification error on the validation set smallest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# get data\n",
    "train_req = requests.get(\"http://work.caltech.edu/data/in.dta\")\n",
    "train = np.array([row.split() for row in train_req.text.split(\"\\r\\n\")[:-1]]).astype(float)\n",
    "\n",
    "test_req = requests.get(\"http://work.caltech.edu/data/out.dta\")\n",
    "test = np.array([row.split() for row in test_req.text.split(\"\\r\\n\")[:-1]]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainValidate(training, validating, testing):\n",
    "    # using validation for determining number of features to use\n",
    "    val_errors = np.zeros(shape = (5,2))\n",
    "    out_errors = np.zeros(shape = 5)\n",
    "    for k in range(3, 8):\n",
    "        val_test = LinearRegression(training, validating, k)\n",
    "        val_test.regress()\n",
    "        val_errors[k-3] = val_test.classificationError()\n",
    "\n",
    "        out_errors[k-3] = val_test.outOfSampleError(testing)\n",
    "\n",
    "\n",
    "    # get index of min of validation and out error, add 3 to get k\n",
    "    return (val_errors, out_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest validation error at k = 6.\n",
      "Smallest out of sample error at k = 7.\n"
     ]
    }
   ],
   "source": [
    "# split data into train and validation \n",
    "validate = train[-10:, :]\n",
    "train = train[:25, :]\n",
    "\n",
    "(val_errors, out_errors) = trainValidate(train, validate, test)\n",
    "\n",
    "\n",
    "print(\"Smallest validation error at k = {}.\".format(np.argmin(val_errors[:, 1]) + 3))\n",
    "print(\"Smallest out of sample error at k = {}.\".format(np.argmin(out_errors) + 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44  0.3 ]\n",
      " [ 0.32  0.5 ]\n",
      " [ 0.08  0.2 ]\n",
      " [ 0.04  0.  ]\n",
      " [ 0.04  0.1 ]]\n",
      "[ 0.42   0.416  0.188  0.084  0.072]\n"
     ]
    }
   ],
   "source": [
    "print(val_errors)\n",
    "print(out_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ A1. [d] k = 6 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Evaluate the out-of-sample classification error using out.dta on the 5 models\n",
    "to see how well the validation set predicted the best of the 5 models. For which\n",
    "model is the out-of-sample classification error smallest?\n",
    "\n",
    "__A2. [e] k = 7 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Reverse the role of training and validation sets; now training with the last 10 examples and validating with the first 25 examples. For which model is the classification error on the validation set smallest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest validation error at k = 6.\n",
      "Smallest out of sample error at k = 6.\n"
     ]
    }
   ],
   "source": [
    "# repeat but swap train and validate\n",
    "train = np.array([row.split() for row in train_req.text.split(\"\\r\\n\")[:-1]]).astype(float)\n",
    "test = np.array([row.split() for row in test_req.text.split(\"\\r\\n\")[:-1]]).astype(float)\n",
    "\n",
    "validate_swap = train[:25, :]\n",
    "train_swap = train[-10:, :]\n",
    "(val_errors_swap, out_errors_swap) = trainValidate(train_swap, validate_swap, test)\n",
    "print(\"Smallest validation error at k = {}.\".format(np.argmin(val_errors_swap[:, 1]) + 3))\n",
    "print(\"Smallest out of sample error at k = {}.\".format(np.argmin(out_errors_swap) + 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3. [d] k = 6__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Once again, evaluate the out-of-sample classification error using out.dta on the 5 models to see how well the validation set predicted the best of the 5 models. For which model is the out-of-sample classification error smallest?\n",
    "\n",
    "__A4. [d] k = 6 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What values are closest in Euclidean distance to the out-of-sample classification error obtained for the model chosen in Problems 1 and 3, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.072\n",
      "0.196\n"
     ]
    }
   ],
   "source": [
    "print(out_errors[np.argmin(out_errors)])\n",
    "print(out_errors_swap[np.argmin(out_errors)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ A5. [b] 0.1 0.2 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Let $e_1$ and $e_2$ be independent random variables, distributed uniformly over the interval $[0, 1]$. Let $e = \\text{min}(e_1, e_2)$. The expected values of $e_1$, $e_2$, $e$ are closest to\n",
    "\n",
    "A6. Let $X = \\text{min} (e_1, e_2)$; to solve for its expected value, we first need its CDF,\n",
    "\n",
    "$$F(x) = \\mathbb{P}(X \\leq x) = 1 - \\mathbb{P}(X > x) = 1 - \\mathbb{P}(\\min(e_1, e_2) > x)$$\n",
    "\n",
    "Clearly the probability that the minimum of two variables is greater than $x$ is the joint probability that both variables are greater than $x$,\n",
    "\n",
    "$$F(x) = 1 - \\mathbb{P}(e_1 > x, e_2 > x) $$\n",
    "\n",
    "and because of independence, \n",
    "\n",
    "$$F(x) = 1 - \\mathbb{P}(e_1 > x) \\mathbb{P} (e_2 > x)  = 1 - \\mathbb{P}(e_1 > x)^2$$\n",
    "\n",
    "and where, because $e_1$ is uniformly distributed over $[0,1]$, we have,\n",
    "\n",
    "$$F(x) = 1 - \\left ( \\frac{x - 1}{0 - 1} \\right )^2 = 1 - (1-x)^2 $$\n",
    "\n",
    "differentiating to get the density, \n",
    "\n",
    "$$ f(x) = 2(1-x) $$\n",
    "\n",
    "and then finally using the pdf to comptue the expected value, \n",
    "\n",
    "$$ \\mathbb{E}[x] = \\int_{0}^{1} x f(x) dx = 2 \\int_{0}^{1} x - x^2 dx = 2 \\left [ \\frac{x^2}{2} - \\frac{x^3}{3} \\right ]_{0}^{1}  =  2(1 / 2 - 1/3) = 1/3 = 0.\\bar{3}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__ A6. [d] 0.5, 0.5, 0.4 __\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. You are given the data points $(x, y): (âˆ’1, 0),(\\rho, 1),(1, 0), \\rho \\geq 0$, and a choice between two models: constant $\\{ h_0(x) = b \\}$ and linear $\\{ h_1(x) = ax + b \\}$.\n",
    "For which value of $\\rho$ would the two models be tied using leave-one-out cross-validation with the squared error measure?\n",
    "\n",
    "A7. For the constant model, we just average the $y$s to solve for $b$. For the linear model, we have two points so we can just solve for the line between them. So we choose 2 points to create the model, and use the squared error of the 3rd point to estimate the model and repeat. \n",
    "\n",
    "So for the constant model: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate = $(-1, 0)$\n",
    "\n",
    "train = $(\\rho, 1), (1, 0)$\n",
    "\n",
    "So that, $h_1(x) = 0.5$ and $h_1(-1) = 0$, with squared error $0.25$. \n",
    "\n",
    "For $h_2$: \n",
    "\n",
    "$a = \\frac{1}{\\rho - 1} $\n",
    "\n",
    "$b = y - ax = 0 - \\frac{1}{\\rho - 1} = -\\frac{1}{\\rho - 1} $\n",
    "\n",
    "$ h_2(-1) = - \\frac{1}{\\rho-1} - \\frac{1}{\\rho-1} = -\\frac{2}{\\rho -1} $, with squared error $\\frac{4}{(\\rho - 1)^2}$\n",
    "\n",
    "Repeat with validate = $(\\rho, 1)$\n",
    "\n",
    "train = $(-1, 0), (1, 0)$\n",
    "\n",
    "$h_1(x) = 0$ and $h_1(\\rho) = 0$, with squared error $1$. \n",
    "\n",
    "$h_2$:\n",
    "\n",
    "$a = \\frac{0}{2} = 0$\n",
    "\n",
    "$b = 0 - 0 = 0$\n",
    "\n",
    "$h_2(\\rho) = 0$ with squared error $1$. \n",
    "\n",
    "Finally, with validate point = $(1, 0)$\n",
    "\n",
    "train = $(-1, 0), (\\rho, 1)$\n",
    "\n",
    "$h_1(x) = 0.5$, $h_1(1) = 0.5$ with squared error $0.25$. \n",
    "\n",
    "$h_2$:\n",
    "\n",
    "$a = \\frac{1}{\\rho + 1} $\n",
    "\n",
    "$b = 0 - (- \\frac{1}{\\rho + 1}) = \\frac{1}{\\rho+1}$\n",
    "\n",
    "$h_2(x) = \\frac{a}{\\rho+1} + \\frac{1}{\\rho+1}$, $h_2(1) = \\frac{2}{\\rho+1}$ with squared error, $\\frac{4}{(\\rho+1)^2} $\n",
    "\n",
    "So that the squared error, for setting the sum of the squared errors equal, \n",
    "\n",
    "$$0.25 + 1 + 0.25 = \\frac{4}{(\\rho -1)^2} + 1 + \\frac{4}{(\\rho+1)^2}$$\n",
    "\n",
    "$$ 0.5 =  \\frac{4}{(\\rho -1)^2} + \\frac{4}{(\\rho+1)^2}$$\n",
    "\n",
    "$$ (\\rho^2 - 1)^2 = 8 (\\rho + 1)^2 + 8 (\\rho - 1)^2$$\n",
    "\n",
    "$$ \\rho^4 - 2\\rho^2 + 1 = 8(\\rho^2 + 2 \\rho + 1) + 8 (\\rho^2 - 2\\rho + 1)$$\n",
    "\n",
    "$$ \\rho^4 - 2 \\rho^2 + 1 = 16\\rho^2 + 16 $$\n",
    "\n",
    "$$ \\rho^4 - 18 \\rho^2 - 15 = 0$$\n",
    "\n",
    "$$ \\rho = \\sqrt{9 + 4 \\sqrt{6}} $$ for $\\rho \\geq 0$\n",
    "\n",
    "__ [c]  $\\sqrt{9 + 4 \\sqrt{6}}$ __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notice: Quadratic Programming packages sometimes need tweaking and have numerical issues, and this is characteristic of packages you will use in practical ML situations. Your understanding of support vectors will help you get to the correct answers._\n",
    "\n",
    "In the following problems, we compare PLA to SVM with hard margin on linearly separable data sets. For each run, you will create your own target function $f$ and data set $\\mathcal{D}$. Take $d = 2$ and choose a random line in the plane as your target function $f$ (do this by taking two random, uniformly distributed points on $[âˆ’1, 1] \\times [âˆ’1, 1]$\n",
    "and taking the line passing through them), where one side of the line maps to $+1$ and the other maps to $âˆ’1$. Choose the inputs $x_n$ of the data set as random points in $\\mathcal{X} = [âˆ’1, 1] \\times [âˆ’1, 1]$, and evaluate the target function on each $x_n$ to get the corresponding output $y_n$. If all data points are on one side of the line, discard the\n",
    "run and start a new run. \n",
    "\n",
    "Start PLA with the all-zero vector and pick the misclassified point for each PLA iteration at random. Run PLA to find the final hypothesis $g_{\\text{PLA}}$ and measure the disagreement between $f$ and\n",
    "$g_{\\text{PLA}}$ as $\\mathbb{P}[f(x) \\neq g_{\\text{PLA}}(x)]$ (you can either calculate this exactly, or approximate it by generating a sufficiently large, separate set of points to evaluate it). Now, run SVM on the same data to find the final hypothesis $g_{\\text{SVM}}$ by solving\n",
    "\n",
    "\\begin{align}\n",
    "&\\min_{w, b} \\quad \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} \\\\\n",
    "&s.t. \\quad y_n(\\mathbf{w}^T \\mathbf{x}_n + b) \\geq 1 \n",
    "\\end{align}\n",
    "\n",
    "using quadratic programming on the primal or the dual problem. Measure the disagreement between $f$ and $g_{\\text{SVM}}$ as $\\mathbb{P}[f(\\mathbf{x}) \\neq g_{\\text{SVM}}(\\mathbf{x})]$, and count the number of support vectors you get in each run. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve the above optimization problem using Lagrange Multipliers; i.e., get the Lagrangian and set its gradient equal to zero then substitute back to the get the dual formulation of the problem:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} - \\sum_{i=1}^{N} \\alpha_i [ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1] $$\n",
    "\n",
    "$$ \\nabla \\mathcal{L} (\\mathbf{w}, b, \\alpha) = \\mathbf{0} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i = 0 $$\n",
    "\n",
    "or, \n",
    "\n",
    "$$\\mathbf{w} = \\sum_{i}^{N} \\alpha_i y_i \\mathbf{x}_i $$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^N \\alpha_i y_i = 0 $$\n",
    "\n",
    "or, \n",
    "\n",
    "$$ \\sum_{i=1}^{N} a_i y_i = 0 $$\n",
    "\n",
    "Substituting these equations back into the original Lagrangian, \n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2} (\\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i) \\cdot (\\sum_{j=1}^{N} \\alpha_j y_j \\mathbf{x}_j) -  \\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i( \\sum_{j=1}^{N} \\alpha_j y_j \\mathbf{x}_j) - \\sum_{i=1}^{N} \\alpha_i y_i b + \\sum_{i=1}^{N} \\alpha_i$$\n",
    "\n",
    "$$\\mathcal{L}(\\alpha)  = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}^T_i \\cdot \\mathbf{x}_j $$\n",
    "\n",
    "so that we can maximize with respect only to $\\alpha$ with the constraints $\\alpha_i \\geq 0$ and the previous constraint $\\sum_{i=1}^{N} \\alpha_i y_i = 0$. \n",
    "\n",
    "Then to get this in terms that cvxopt will understand, we need to minimize the negative of this, i.e., \n",
    "\n",
    "$$ \\min_a \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} y_i y_j \\alpha_i \\alpha_j \\mathbf{x}_i^T \\mathbf{x}_j - \\sum_{i = 1}^N \\alpha_i $$\n",
    "\n",
    "So the quadratic coefficients form an $N \\times N$ matrix $Q$ with entries $p[i][j] = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $, and the linear coefficients are just a $N$ dimensional vector of $-1$s (with the constraints as above). \n",
    "\n",
    "Explicitly, once the QP gives us the alphas, non-zero values represent the indices of support vectors, we can use the alphas to solve for the weights, and then use any support vector to solve for the constant bias $b$, since,\n",
    "\n",
    "$$y_i (\\mathbf{w}^T \\mathbf{x}_i + b) = 1 $$\n",
    "$$y_i \\mathbf{w}^T \\mathbf{x}_i + y_i b = 1 $$\n",
    "\n",
    "Since $y_i \\in \\{ +1, -1 \\}$, $1 / y_i = y_i$ and $(y_i)^2 = 1$, so, \n",
    "\n",
    "$$ b = \\frac{1}{y_i} (1 - y_i \\mathbf{w}^T x_i) = y_i - \\mathbf{w}^T \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# quadratic programming package\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "class SVM():\n",
    "    ''' Support Vector Machine Model compared to Perceptron Learning Algorithm on random points in R2'''\n",
    "    def __init__(self, N):\n",
    "        ''' generate target function and N random points; initialize weights'''\n",
    "        # number of data points\n",
    "        self.N = N\n",
    "        \n",
    "        self.getPoints()\n",
    "        \n",
    "        # test if all values are on one side or another of the target\n",
    "        while np.all(self.y == 1) or np.all(self.y == -1):\n",
    "            # if so, reinitialize target and points\n",
    "            self.getPoints()\n",
    "            \n",
    "        # initialize weights to zero\n",
    "        self.w = np.zeros(2)\n",
    "        \n",
    "        # initialize bias to zero\n",
    "        self.b = 0\n",
    "\n",
    "    def getPoints(self):\n",
    "         # get two points to determine target function line\n",
    "        p = np.random.uniform(-1, 1, (2, 2))\n",
    "        \n",
    "        # get equation of that line\n",
    "        m = (p[1][1] - p[0][1]) / (p[1][0] - p[0][0])\n",
    "        \n",
    "        b = p[1][1] - m * p[1][0]\n",
    "        \n",
    "        # target function/line\n",
    "        t = lambda x: m * x[0] + b\n",
    "        \n",
    "        # target applied to full array\n",
    "        self.target = lambda x: np.where(np.apply_along_axis(t, 1, x) > 0, 1, -1)\n",
    "        \n",
    "        # generate N data points\n",
    "        self.x = np.random.uniform(-1, 1, (self.N, 2))\n",
    "        \n",
    "        # apply target to data to get proper classified ys\n",
    "        self.y = self.target(self.x)\n",
    "        \n",
    "    def PLA_test(self):\n",
    "        ''' run perceptron learning algorithm with same data, return disagreement between target and hypothesis '''\n",
    "        pla = PLA(self.N, self.target, self.x, self.y)\n",
    "        return pla.run()\n",
    "\n",
    "    \n",
    "    def SVM_run(self):\n",
    "        ''' run support vector machine'''\n",
    "        # formulate data into quadratic programming constraints to get lagrange multipliers\n",
    "        # gram matrix of data (i.e., matrix of all inner products)\n",
    "        gram = np.dot(self.x, self.x.T)\n",
    "        # quadratic terms\n",
    "        P = matrix(np.outer(self.y,self.y) * gram, tc = 'd')\n",
    "        # linear terms \n",
    "        q = matrix(-np.ones(self.N), tc = 'd')\n",
    "        # constraints\n",
    "        A = matrix(self.y, (1,self.N), tc = 'd')\n",
    "        b = matrix(0.0)\n",
    "        \n",
    "        G = matrix(np.diag(np.ones(self.N) * -1), tc = 'd')\n",
    "        h = matrix(np.zeros(self.N), tc = 'd')\n",
    "        \n",
    "        # make cxvopt solver silent \n",
    "        solvers.options['show_progress'] = False\n",
    "        \n",
    "        # get solution\n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        \n",
    "        # Lagrange multipliers\n",
    "        alpha = np.array(sol['x'])\n",
    "        \n",
    "        # boolean mask for support vectors\n",
    "        sv = np.where(alpha > 10**-4)[0]\n",
    "        \n",
    "        # if no support vectors; all points are to one side\n",
    "        if sv.shape[0] == 0:\n",
    "            # start over with new data\n",
    "            print(\"Init Error: all points to one side. \")\n",
    "            \n",
    "        # solve for weights (elementwise multiply alpha*y*x then sum)\n",
    "        self.w = np.sum((alpha[sv].T * self.y[sv]).T[np.newaxis] * self.x[sv], axis = 1)\n",
    "            \n",
    "        # solve for bias using first support vector\n",
    "        self.b = self.y[sv[0]] - np.inner(self.w, self.x[sv[0]])\n",
    "        \n",
    "        # return support vectors\n",
    "        return self.x[sv]\n",
    "\n",
    "    def SVM_test(self):\n",
    "        ''' run SVM, generate testing data, approximate out of sample error'''\n",
    "        # generate random testing points\n",
    "        test_x = np.random.uniform(-1, 1, (1000, 2))\n",
    "        \n",
    "        # classify with actual target\n",
    "        test_y = self.target(test_x)\n",
    "        \n",
    "        # compare classification with SVM prediction\n",
    "        return test_y[(test_y != self.classify(test_x))[0]].shape[0] / test_y.shape[0]\n",
    "        \n",
    "    def classify(self, data):\n",
    "        ''' use SVM to predict on data '''\n",
    "        return np.where(np.inner(self.w, data) > 0, 1, -1)\n",
    "    \n",
    "    def compare(self):\n",
    "        ''' run both algorithms, compare out of sample error approximations '''\n",
    "        # run SVM/get number of support vectors\n",
    "        supports = self.SVM_run()\n",
    "        \n",
    "        # compare performance of SVM and PLA, return 1 if SVM is better\n",
    "        if self.SVM_test() > self.PLA_test():\n",
    "            return (1, supports)\n",
    "        else:\n",
    "            return (0, supports)\n",
    "    \n",
    "class PLA():\n",
    "    ''' Perceptron Learning Algorithm (for comparison)'''\n",
    "    def __init__(self, N, target, data, y):\n",
    "        self.N = N\n",
    "        self.target = target\n",
    "        \n",
    "        self.x = np.concatenate((np.ones(data.shape[0]).reshape(data.shape[0], 1), data), axis = 1)\n",
    "        self.y = y\n",
    "        \n",
    "        self.w = np.zeros(3)\n",
    "        \n",
    "    def h(self, x):\n",
    "        return np.where(np.inner(self.w.T, x) > 0, 1, -1)\n",
    "\n",
    "    def update(self):\n",
    "        ''' apply one iteration of PLA '''\n",
    "        hyp = self.h(self.x)\n",
    "        \n",
    "        # get all misclassified points, multiply by what direction it's wrong in\n",
    "        missed = (hyp != self.y)\n",
    "        ys = self.y[missed]\n",
    "        \n",
    "        if len(ys) == 0:\n",
    "            # if nothing misclassified, learning complete\n",
    "            return True\n",
    "        else:\n",
    "            # randomly choose a wrong point, use it to update weights\n",
    "            wrong_index = np.random.randint(len(ys))\n",
    "            self.w += ys[wrong_index] * self.x[missed][wrong_index]\n",
    "            return False\n",
    "        \n",
    "    def run(self):\n",
    "        ''' run PLA to completion, return probability of error '''\n",
    "        while (not self.update()):\n",
    "            pass\n",
    "        \n",
    "        # generate testing set\n",
    "        test_x = np.random.uniform(-1, 1, (1000, 2))\n",
    "        test_y = self.target(test_x)\n",
    "        \n",
    "        test_x = np.concatenate((np.ones(1000).reshape(1000, 1), test_x), axis = 1)\n",
    "        \n",
    "        return (test_y[self.h(test_x) != test_y].shape[0]) / test_y.shape[0]\n",
    "\n",
    "                                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712\n"
     ]
    }
   ],
   "source": [
    "# counter for number of times SVM beats PLA\n",
    "gsvm = 0\n",
    "\n",
    "# run experiment 1000 times\n",
    "for i in range(1000):\n",
    "    t = SVM(10)\n",
    "    # unpack tuple of GSVM and supports (no question for that here)\n",
    "    (g, _) = t.compare()\n",
    "    \n",
    "    gsvm += g\n",
    "\n",
    "print(gsvm / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For N = 10, repeat the above experiment for 1000 runs. How often is $g_{SVM}$\n",
    "better than $g_{PLA}$ in approximating $f$? The percentage of time is closest to:\n",
    "\n",
    "__ A8. [d] 80% __\n",
    "\n",
    "_should be [c] here apparently, but does not agree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "3.019\n"
     ]
    }
   ],
   "source": [
    "# same but for N = 100 and counting support vectors\n",
    "supports = np.empty(1000, dtype = int)\n",
    "\n",
    "# counter for number of times SVM beats PLA\n",
    "gsvm = 0\n",
    "\n",
    "# run experiment 1000 times\n",
    "for i in range(1000):\n",
    "    t = SVM(100)\n",
    "    \n",
    "    # unpack tuple\n",
    "    (g, s) = t.compare()\n",
    "    gsvm += g\n",
    "    supports[i] = s.shape[0]\n",
    "\n",
    "print(gsvm / 1000)\n",
    "print(np.average(supports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. For $N = 100$, repeat the above experiment for $1000$ runs. How often is $g_{SVM}$\n",
    "better than $g_{PLA}$ in approximating $f$? The percentage of time is closest to:\n",
    "\n",
    "__ A9. [e] 90% __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. For the case $N = 100$, which of the following is the closest to the average number\n",
    "of support vectors of $g_{SVM}$ (averaged over the $1000$ runs)?\n",
    "\n",
    "__ A10. [b] 3__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    ''' Linear Regression Learning Model (copied from HW6)'''\n",
    "    def __init__(self, train, test, k = 2):\n",
    "        ''' assuming data is formatted so that last row are ys, rest is xs'''\n",
    "        # number of features to use\n",
    "        self.k = k+1\n",
    "        self.x = train[:, :-1]\n",
    "        \n",
    "        # add column of 1s to data\n",
    "        self.x = np.concatenate((np.ones(train.shape[0]).reshape(train.shape[0], 1), self.x), axis =1)\n",
    "        \n",
    "        # seperate out ys\n",
    "        self.y = train[:, -1]\n",
    "        \n",
    "        # apply non-linear transform; use k features\n",
    "        self.x = self.transform(self.x)[:, :self.k]\n",
    "        \n",
    "        # initialize weights to 0\n",
    "        self.w = np.zeros(self.x.shape[1])\n",
    "        \n",
    "        # seperate and apply transformation to testing data\n",
    "        self.test_x = test[:, :-1]\n",
    "        self.test_x = np.concatenate((np.ones(test.shape[0]).reshape(test.shape[0], 1), self.test_x), axis =1)\n",
    "\n",
    "        self.test_y = test[:, -1]        \n",
    "        self.test_x = self.transform(self.test_x)[:, :self.k]\n",
    "    \n",
    "    def regress(self, l = 0):\n",
    "        ''' solve for regularized weights with lambda = l; defaults to no regularization '''\n",
    "        #self.w = np.linalg.lstsq(np.dot(self.x.T, self.x) + l * np.identity(self.x.shape[1]), np.dot(self.x.T, self.y))[0]\n",
    "        self.w = np.inner(np.linalg.pinv(self.x), self.y)\n",
    "        \n",
    "        \n",
    "    def classify(self, data):\n",
    "        ''' hard threshold regression for classification '''\n",
    "        return np.where(np.inner(self.w, data) > 0, 1.0, -1.0)\n",
    "    \n",
    "    def classificationError(self):\n",
    "        ''' calculate in and out of sample/validation error '''\n",
    "        ein = self.y[self.classify(self.x) != self.y].shape[0]\n",
    "        eout = self.test_y[self.classify(self.test_x) != self.test_y].shape[0]\n",
    "        \n",
    "        # return fraction of misaclassified points in and out of sample\n",
    "        return (ein / self.y.shape[0], eout / self.test_y.shape[0])\n",
    "        \n",
    "    def outOfSampleError(self, data):\n",
    "        ''' use seperate data set to estimate out of sample error '''\n",
    "        out_x = data[:, :-1]\n",
    "        out_x = np.concatenate((np.ones(data.shape[0]).reshape(data.shape[0], 1), out_x), axis =1)\n",
    "\n",
    "        out_y = data[:, -1]        \n",
    "        out_x = self.transform(out_x)[:, :self.k]\n",
    "        \n",
    "        return (out_y[self.classify(out_x) != out_y].shape[0]/ out_y.shape[0])\n",
    "    \n",
    "    def transform(self, data):\n",
    "        ''' applies specified nonlinear transform on data by adding columns'''\n",
    "        trans = self.addFeature(data, np.square(data[:, 1]))\n",
    "        trans = self.addFeature(trans, np.square(data[:, 2]))\n",
    "        trans = self.addFeature(trans, data[:, 1]*data[:, 2])\n",
    "        trans = self.addFeature(trans, np.abs(data[:, 1] - data[:, 2]))\n",
    "        trans = self.addFeature(trans, np.abs(data[:, 1] + data[:, 2]))\n",
    "\n",
    "        return trans\n",
    "\n",
    "    def addFeature(self, data, feature):\n",
    "        ''' helper function for adding feature '''\n",
    "        return np.concatenate((data, feature.reshape(data.shape[0], 1)), axis = 1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
