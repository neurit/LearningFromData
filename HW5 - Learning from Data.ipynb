{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a noisy target $y = \\mathbf{w}^{*T} \\mathbf{x} + \\epsilon$, where $x \\in \\mathbb{R}^d$ (with the added coordinate $x_0 = 1$), $y \\in \\mathbb{R}$, $\\mathbf{w}^∗$ is an unknown vector, and $\\epsilon$ is a noise term with zero mean and $\\sigma^2$ variance. Assume $\\epsilon$ is independent of $x$ and of all other $\\epsilon$’s. If linear regression is carried out using a training data set $\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1), . . . ,(\\mathbf{x}_N , y_N ) \\} $, and outputs the parameter vector $\\mathbf{w}_{\\text{lin}}$, it can be shown that the expected in-sample error $E_{\\text{in}}$ with\n",
    "respect to $\\mathcal{D}$ is given by:\n",
    "\n",
    "$$ \\mathbb{E}_{\\mathcal{D}} [ E_{\\text{in}} (\\mathbf{w}_{\\text{lin}})] = \\sigma^2 \\left ( 1 - \\frac{d+1}{N} \\right ) $$\n",
    "\n",
    "Q1. For $\\sigma = 0.1$ and $d = 8$, which among the following choices is the greatest number of examples N that will result in an expected $E_{\\text{in}}$ greater than $0.008$?\n",
    "\n",
    "A. Just a simple calculation.\n",
    "\n",
    "$$0.008 < \\mathbb{E}_{\\mathcal{D}} [ E_{\\text{in}} (\\mathbf{w}_{\\text{lin}})] = (0.1)^2 \\left ( 1 - \\frac{9}{N} \\right ) $$\n",
    "\n",
    "$$ 0.008 < 0.01 - \\frac{.09}{N} $$\n",
    "\n",
    "$$ \\frac{.09}{N} < 0.002 $$\n",
    "\n",
    "$$ N > \\frac{.09}{0.002} \\approx 45 $$\n",
    "\n",
    "__ A1. [c] 100 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear classification, consider the feature transform $\\Phi : R^2 \\rightarrow R^2$ (plus the added zeroth coordinate) given by:\n",
    "\n",
    "$$ \\Phi(1, x_1, x_2) = (1, x_1^2, x_2^2) $$\n",
    "\n",
    "Q2. Which of the following sets of constraints on the weights in the $\\mathcal{Z}$ space could correspond to the hyperbolic decision boundary in $\\mathcal{X}$ depicted in the figure? \n",
    "\n",
    "<img src= \"figure.png\" style=\"width: 300px\"/>\n",
    "\n",
    "You may assume that $\\tilde{w}_0$ can be selected to acheive the desired boundary. \n",
    "\n",
    "A. The graph matches the general hyperbolic function $x_1^2 - x_2^2 = 1$, but since $x_0 = 1$, to get this graph we actually need to multiply both sides by $-1$ and $-x_1^2 + x_2^2 = -1$. Then, \n",
    "\n",
    "$$\\tilde{w}_0 + \\tilde{w}_1 z_1 + \\tilde{w}_2 z_2 = \\tilde{w}_0 + \\tilde{w}_1 x_1^2 + \\tilde{w}_2 x_2^2 $$\n",
    "\n",
    "So clearly $\\tilde{w}_1 < 0$ and $\\tilde{w}_2 > 0$, \n",
    "\n",
    "__ A2. [d] $\\tilde{w}_1 < 0$, $\\tilde{w}_2 > 0$ __\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider the 4th order polynomial transform from the input space $\\mathbb{R}^2$. \n",
    "\n",
    "$$\\Phi_4: \\mathbf{x} \\rightarrow (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3, x_1^4, x_1^3x_2, x_1^2 x_2^2, x_1 x_2^3, x_2^4)$$\n",
    "\n",
    "There are a total of $d=15$ dimensions transformed space, so, for a linear model, \n",
    "\n",
    "$$d_{VC} \\leq \\tilde{d} = 15$$\n",
    "\n",
    "__A3. [c] 15 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the nonlinear error surface $E(u, v) = (u e_v − 2v e^{−u})^2$. We start at the point $(u, v) = (1, 1)$ and minimize this error using gradient descent in the $uv$ space. Use $\\eta = 0.1$ (learning rate, not step size).\n",
    "\n",
    "Q4. What is the partial derivative of $E(u, v)$ with respect to $u$, i.e., $\\frac{\\partial E}{\\partial u}$?\n",
    "\n",
    "A. Just applying chain rule.\n",
    "$$ \\frac{\\partial E}{\\partial u} = 2(ue^v - 2ve^{-u})(e^v + 2ve^{-u})$$\n",
    "\n",
    "__ A4. [e] __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.How many iterations (among the given choices) does it take for the error $E(u, v)$ to fall below $10^{−14}$ for the first time? In your programs, make sure to use double precision to get the needed accuracy.\n",
    "\n",
    "We also need the partial derivative with respect to $v$, \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial v} = 2(ue^v - 2ve^{-u})(ue^v - 2e^{-u})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vector = (0.045, 0.024); took 10 iterations.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def error(u, v):\n",
    "    return (u * math.exp(v) - 2 * v * math.exp(-u))**2\n",
    "\n",
    "def gradient(u, v):\n",
    "    ''' returns gradient vector of error at point u, v'''\n",
    "    du = 2 * (u * math.exp(v) - 2 * v * math.exp(-u)) * (math.exp(v) + 2 * v * math.exp(-u))\n",
    "    dv = 2 * (u * math.exp(v) - 2 * v * math.exp(-u)) * (u * math.exp(v) - 2 * math.exp(-u))\n",
    "    \n",
    "    return np.array([du, dv])\n",
    "    \n",
    "def gradientDescent(min_error, eta):\n",
    "    ''' runs gradient descent until min_error is met'''\n",
    "    # starting w = (u,v) = (1,1)\n",
    "    w = np.array([1, 1])\n",
    "    iterations = 0\n",
    "    \n",
    "    while (error(w[0], w[1]) > min_error):\n",
    "        w = w - eta * gradient(w[0], w[1])\n",
    "        iterations += 1\n",
    "        \n",
    "    return (iterations, w)\n",
    "        \n",
    "    \n",
    "(count, final) = gradientDescent(10e-14, 0.1)\n",
    "print(\"Final vector = ({:.3f}, {:.3f}); took {} iterations.\".format(final[0], final[1], count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ A5. [d] 10 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. After running enough iterations such that the error has just dropped below $10^{−14}$, what are the closest values (in Euclidean distance) among the following choices to the final $(u, v)$ you got in Problem 5?\n",
    "\n",
    "__A6. [e] $(0.045, 0.024)$ __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Now, we will compare the performance of “coordinate descent.” In each iteration, we have two steps along the 2 coordinates. Step 1 is to move only along the $u$ coordinate to reduce the error (assume first-order approximation holds like in gradient descent), and step 2 is to reevaluate and move only along the $v$ coordinate to reduce the error (again, assume first-order approximation holds). Use the same learning rate of $\\eta = 0.1$ as we did in gradient descent. What will the error $E(u, v)$ be closest to after 15 full iterations (30 steps)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13981379199615315"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coordinateDescent(steps, eta):\n",
    "    ''' gradient descent by updating one coordinate at a time'''\n",
    "    w = np.array([1.0, 1.0])\n",
    "    \n",
    "    while (steps != 0):\n",
    "        # odd or even for alternating coordinate \n",
    "        if steps % 2 == 0:\n",
    "            w[0] = w[0] - eta * gradient(w[0], w[1])[0]\n",
    "            steps -= 1\n",
    "        else:\n",
    "            w[1] = w[1] - eta * gradient(w[0], w[1])[1]\n",
    "            steps -= 1\n",
    "            \n",
    "    return error(w[0], w[1])\n",
    "    \n",
    "coordinateDescent(30, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ A7. [a] $10^{-1}$ __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    ''' Logistic Regression  '''\n",
    "    def __init__(self, N = 100, eta = 0.01):\n",
    "        # number of training examples\n",
    "        self.N = N\n",
    "        self.epochs = 0\n",
    "        self.eta = eta\n",
    "        \n",
    "        # randomly generate target function from two points\n",
    "        p = np.random.uniform(-1, 1, (2, 2))\n",
    "        m = (p[1][1] - p[0][1]) / (p[1][0] - p[0][0])\n",
    "        b = p[1][1] - m * p[1][0]\n",
    "        \n",
    "        # line equation\n",
    "        t = lambda x: m*x[1] + b\n",
    "        \n",
    "        # equation applied per column\n",
    "        tar = lambda x: np.apply_along_axis(t, 1, x)\n",
    "\n",
    "        # full target function that maps to 1 or -1\n",
    "        self.target = lambda x: np.where(tar(x) - x[:, 1] > 0, 1, -1)\n",
    "        \n",
    "        # generate random data points\n",
    "        self.x = np.random.uniform(-1, 1, (self.N, 2))\n",
    "    \n",
    "        # add column of zeros\n",
    "        self.x = np.concatenate((np.repeat(1,N).reshape(N, 1), self.x), axis=1)\n",
    "\n",
    "        # classified points\n",
    "        self.y = self.target(self.x)\n",
    "        \n",
    "        # initial weights to zero\n",
    "        self.w = np.zeros(3)\n",
    "        \n",
    "\n",
    "    def signal(self):\n",
    "        return np.inner(self.w.T, self.x)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def h(self):\n",
    "        self.sigmoid(self.signal())\n",
    "    \n",
    "    def crossEntropy(self, data):\n",
    "        ''' evaluate cross entropy error of data (testing or training) '''\n",
    "        ys = self.target(data)\n",
    "        return np.average(np.log(1 + np.exp(-ys * np.inner(self.w.T, data))))\n",
    "    \n",
    "    def grad(self, n):\n",
    "        ''' gradient at just a single point (x_n, y_n)'''\n",
    "        return -self.y[n] * self.x[n]*self.sigmoid(-self.y[n]*np.inner(self.w.T, self.x[n]))\n",
    "    \n",
    "\n",
    "    def stochasticGradientDescent(self):\n",
    "        ''' run stochastic gradient descent through one epoch: returns boolean of whether done or not'''\n",
    "        # for each epoch, get an array of indices, shuffle them\n",
    "        indices = np.arange(self.N)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # store initial weight copy to measure progress\n",
    "        initial_weight = np.copy(self.w)\n",
    "        \n",
    "        for n in indices:\n",
    "            self.w = self.w - self.eta * self.grad(n)\n",
    "        \n",
    "        self.epochs += 1\n",
    "        \n",
    "        # check if done\n",
    "        if np.linalg.norm(self.w - initial_weight) < 0.01:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def approximateEout(self):\n",
    "        ''' approximate out of sample error using separate testing data '''\n",
    "        # testing data\n",
    "        test = np.random.uniform(-1, 1, 1000)\n",
    "        \n",
    "        # classify it using target function\n",
    "        y = self.target(test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average eout is 0.08700039158734597\n",
      "Average epochs to converge is 301.71\n"
     ]
    }
   ],
   "source": [
    "epoch = np.empty(100)\n",
    "eout = np.empty(100)\n",
    "ein = np.empty(100)\n",
    "for i in range(100):\n",
    "    test = LogisticRegression()\n",
    "    \n",
    "    while not test.stochasticGradientDescent():\n",
    "        continue\n",
    "    \n",
    "    data = np.random.uniform(-1, 1, (100, 2))\n",
    "    data = np.concatenate((np.repeat(1,100).reshape(100, 1), data), axis=1)\n",
    "    eout[i] = test.crossEntropy(data)\n",
    "    ein[i] = test.crossEntropy(test.x)\n",
    "    epoch[i] = test.epochs\n",
    "\n",
    "print(\"Average eout is {}\".format(np.average(eout)))\n",
    "print(\"Average epochs to converge is {}\".format(np.average(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Which of the following is closest to $E_{\\text{out}}$ for $N = 100$?\n",
    "\n",
    "__ A8. [d] 0.100 __ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. How many epochs does it take on average for Logistic Regression to converge for $N = 100$ using the above initialization and termination rules and the specified learning rate? Pick the value that is closest to your results.\n",
    "\n",
    "__ A9. [a] 350 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. The Perceptron Learning Algorithm can be implemented as SGD using which of the following error functions $e_n(\\mathbf{w})$? Ignore the points $\\mathbf{w}$ at which $e_n(w)$ is not twice differentiable. \n",
    "\n",
    "A. The error function should in some sense encode the error function we previously used, i.e., represent the number of misclassified points. \n",
    "\n",
    "In general, if a point $x_n$ is misclassified, \n",
    "\n",
    "$y_n \\mathbf{w}^T x_n < 0$, \n",
    "\n",
    "in other words, if $y_n \\mathbf{w}^T \\mathbf{x}_n > 0$, we want the error to be 0, and if $y_n \\mathbf{w}^T \\mathbf{x}_n < 0$, we want a positive error. This is well encoded in $- \\text{min} (0, y_n \\mathbf{x}^T \\mathbf{x}_n)$\n",
    "\n",
    "__ A10. [e] $- \\text{min} (0, y_n \\mathbf{x}^T \\mathbf{x}_n)$ __\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
