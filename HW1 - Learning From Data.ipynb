{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What types of Machine Learning, if any, best describe the following three scenarios:\n",
    "\n",
    "(i) A coin classification system is created for a vending machine. The developers obtain exact coin specifications from the U.S. Mint and derive\n",
    "a statistical model of the size, weight, and denomination, which the vending machine then uses to classify coins.\n",
    "\n",
    "(ii) Instead of calling the U.S. Mint to obtain coin information, an algorithm is presented with a large set of labeled coins. The algorithm uses this data to infer decision boundaries which the vending machine then uses to classify its coins.\n",
    "\n",
    "(iii) A computer develops a strategy for playing Tic-Tac-Toe by playing repeatedly\n",
    "and adjusting its strategy by penalizing moves that eventually lead\n",
    "to losing.\n",
    "\n",
    "__A. [d] (i) Not learning, (ii) Supervised Learning, (iii) Reinforcement Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Which of the following problems are best suited for Machine Learning?\n",
    "\n",
    "(i) Classifying numbers into primes and non-primes.\n",
    "\n",
    "(ii) Detecting potential fraud in credit card charges.\n",
    "\n",
    "(iii) Determining the time it would take a falling object to hit the ground.\n",
    "\n",
    "(iv) Determining the optimal cycle for traffic lights in a busy intersection.\n",
    "\n",
    "__A. [a] (ii) and (iv)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. We have 2 opaque bags, each containing 2 balls. One bag has 2 black balls and the other has a black ball and a white ball. You pick a bag at random and then pick one of the balls in that bag at random. When you look at the ball, it is black. You now pick the second ball from that same bag. What is the probability that this ball is also black?\n",
    "\n",
    "Simple answer: In the conditional universe, of the 3 scenarios where the first ball you draw is black, two of them result in you picking a second black ball, so 2/3. \n",
    "\n",
    "Longer answer:\n",
    "Let C be the event that you choose the bag with two black balls, and D be the event that you choose the other with one black and one white. \n",
    "\n",
    "$\\mathbb{P}(C) = 0.5$ and $\\mathbb{P}(D) = 0.5$\n",
    "\n",
    "Let $F_B$ be the event that the first ball you look at from the bag is black, and $S_B$ be the event that the second ball you look at is black. \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\mathbb{P}(S_B | F_B) = \\frac{\\mathbb{P}(S_B \\cap F_B)}{\\mathbb{P}(F_B)}  = \\frac{\\mathbb{P}(B)}{\\mathbb{P}(F_B|C) \\mathbb{P}(C) + \\mathbb{P}(F_B | D) \\mathbb{P}(D)} = \\frac{0.5}{0.5 + 0.5(0.5)} = 2/3$$\n",
    "\n",
    "__A. [d] 2/3__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sample of 10 marbles drawn from a bin containing red and green marbles.\n",
    "The probability that any marble we draw is red is µ = 0.55 (independently, with\n",
    "replacement). We address the probability of getting no red marbles (ν = 0) in the\n",
    "following cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. We draw only one such sample. Compute the probability that ν = 0. The\n",
    "closest answer is (‘closest answer’ means: |your answer−given option| is closest\n",
    "to 0):\n",
    "\n",
    "No red marbles is the same as getting 10 green marbles, all with probability $1-\\mu$ since each draw is independent, so \n",
    "\n",
    "$\\nu = (1-\\mu)^{10} = (0.45)^{10} = 0.00034050628$\n",
    "\n",
    "__[b] $3.405 \\times 10^{-4}$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. We draw 1,000 independent samples. Compute the probability that (at least)\n",
    "one of the samples has ν = 0. The closest answer is:\n",
    "\n",
    "Simple answer: 1000 * above. \n",
    "\n",
    "Longer: Let event $A$ denote the event that at least one sample has $\\nu = 0$. Probability that an individual sample has $\\nu = 0$ is given above; denote this by $A_i$ for each sample $i$ so $A = \\bigcup_{i=1}^{1000} A_i$. Since samples are disjoint,\n",
    "\n",
    "$$\\mathbb{P}(A) = \\mathbb{P}(\\bigcup_{i=1}^{1000} A_i) = \\sum_{i=1}^{1000} \\mathbb{P}(A_i) = 1000(3.405 \\times 10^{-4}) = 0.3405$$\n",
    "\n",
    "to which the closest value is, \n",
    "\n",
    "__[c] 0.289__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Boolean target function over a 3-dimensional input space $X = \\{0, 1\\}^2$ \n",
    "(instead of our $\\pm1$ binary convention, we use 0,1 here since it is standard for Boolean\n",
    "functions). We are given a data set $D$ of five examples represented in the table below,\n",
    "where $y_n = f(x_n)$ for $n = 1, 2, 3, 4, 5.$\n",
    "\n",
    "\n",
    "$x_n$ |$y_n$ \n",
    "------|---\n",
    "0 0 0 | 0\n",
    "0 0 1 | 1\n",
    "0 1 0 | 1\n",
    "0 1 1 | 0\n",
    "1 0 0 | 1\n",
    "Note that in this simple Boolean case, we can enumerate the entire input space (since\n",
    "there are only $2^3 = 8$ distinct input vectors), and we can enumerate the set of all\n",
    "possible target functions (there are only $2^{2^3} = 256$ distinct Boolean function on 3\n",
    "Boolean inputs).\n",
    "\n",
    "\n",
    "Let us look at the problem of learning $f$. Since $f$ is unknown except inside $D$, any\n",
    "function that agrees with $D$ could conceivably be $f$. Since there are only 3 points in\n",
    "$X$ outside $D$, there are only $2^3 = 8$ such functions.\n",
    "\n",
    "\n",
    "The remaining points in $X$ which are not in $D$ are: 101, 110, and 111. We want to\n",
    "determine the hypothesis that agrees the most with the possible target functions. In\n",
    "order to quantify this, count how many of the 8 possible target functions agree with\n",
    "each hypothesis on all 3 points, how many agree on just 2 of the points, on just 1\n",
    "point, and how many do not agree on any points. The final score for each hypothesis\n",
    "is computed as follows:\n",
    "\n",
    "Score = (# of target functions agreeing with hypothesis on all 3 points)×3 + (#\n",
    "of target functions agreeing with hypothesis on exactly 2 points)×2 + (# of target\n",
    "functions agreeing with hypothesis on exactly 1 point)×1 + (# of target functions\n",
    "agreeing with hypothesis on 0 points)×0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Which hypothesis g agrees the most with the possible target functions in terms\n",
    "of the above score?\n",
    "\n",
    "A. The eight possibilites for the target function are $000$, $001$, $010$, $011$, $100$, \n",
    "\n",
    "$g$ returns $1$ for all three points yields $111$. There is 1 target function that agrees with all three points, 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will create your own target function $f$ and data set $D$ to see how the Perceptron Learning Algorithm works. Take $d = 2$ so you can visualize the problem, and assume $X = [−1, 1] \\times [−1, 1]$ with uniform probability of picking each $x \\in X$ .\n",
    "In each run, choose a random line in the plane as your target function $f$ (do this by taking two random, uniformly distributed points in $[−1, 1] \\times [−1, 1]$ and taking the line passing through them), where one side of the line maps to $+1$ and the other maps to $−1$. Choose the inputs $x_n$ of the data set as random points (uniformly in $X$), and evaluate the target function on each $x_n$ to get the corresponding output $y_n$.\n",
    "Now, in each run, use the Perceptron Learning Algorithm to find $g$. Start the PLA with the weight vector $\\mathbf{w}$ being all zeros (consider sign(0) = 0, so all points are initially misclassified), and at each iteration have the algorithm choose a point randomly from the set of misclassified points. We are interested in two quantities: the number\n",
    "of iterations that PLA takes to converge to $g$, and the disagreement between $f$ and $g$ which is $\\mathbb{P}[f(x) \\neq g(x)]$ (the probability that $f$ and $g$ will disagree on their classification of a random point). You can either calculate this probability exactly, or approximate it by generating a sufficiently large, separate set of points to estimate it.\n",
    "In order to get a reliable estimate for these two quantities, you should repeat the experiment for 1000 runs (each run as specified above) and take the average over\n",
    "these runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Perceptron:\n",
    "    ''' Perceptron Learning Algorithm implemented on random (uniform) 2D data '''\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        # number of training examples\n",
    "        self.N = N\n",
    "        \n",
    "        # generate random target function from two points (y = m*x + b)\n",
    "        p = np.random.uniform(0, 1, (2,2))\n",
    "        m = (p[1][1] - p[0][1]) / (p[1][0] - p[0][0])\n",
    "        b = p[1][1] - m * p[1][0]\n",
    "        self.target = lambda x: m * x[0] + b\n",
    "        \n",
    "        # generate random data x_n\n",
    "        self.x = np.random.uniform(0, 1, (N, 2))\n",
    "\n",
    "        # y_n = 1 if f(x_0) - x_1 > 0, -1 otherwise\n",
    "        self.y = self.classify(self.x)\n",
    "        \n",
    "        # append column vector x_0 set to 1 to account for threshhold\n",
    "        self.x = np.concatenate((np.repeat(1,N).reshape(N, 1), self.x), axis=1)\n",
    "        \n",
    "        # initialize weights to zero (3 weights because of x_0)\n",
    "        self.w = np.zeros((3))\n",
    "    \n",
    "        # count of iterations\n",
    "        self.iterations = 0 \n",
    "\n",
    "    def classify(self, data):\n",
    "        ''' applies target function to get true classification '''\n",
    "        return np.where((np.apply_along_axis(self.target, 1, data) - data[:, 1]) > 0, 1, -1)\n",
    "\n",
    "    def h(self, data):\n",
    "        ''' returns hypothesis function applied to data as numpy array '''\n",
    "        # get inner product\n",
    "        inner = np.inner(self.w.T, data)\n",
    "        \n",
    "        # remap to 1 or -1 if positive or non-positive\n",
    "        inner = np.where(inner > 0, int(1), int(-1))\n",
    "        return inner\n",
    "        \n",
    "    \n",
    "    def update(self):\n",
    "        ''' apply one iteration of learning '''\n",
    "        # increment iterations\n",
    "        self.iterations += 1\n",
    "        \n",
    "        hyp = self.h(self.x)\n",
    "        # get all misclassified points, multiply by what direction it's wrong in\n",
    "        #missed = [self.y[i]*self.x[i] for i in range(self.N) if self.h()[i]!= self.y[i]] \n",
    "        missed = (hyp != self.y)\n",
    "        ys = self.y[missed]\n",
    "        if len(ys) == 0:\n",
    "            # if nothing misclassified, learning complete\n",
    "            return True\n",
    "        else:\n",
    "            # randomly choose a wrong point, use it to update weights\n",
    "            wrong_index = np.random.randint(len(ys))\n",
    "            self.w += ys[wrong_index] * self.x[missed][wrong_index]\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        ''' run algorithm until converges or exceed 100000 iterations '''\n",
    "        while (not self.update()):\n",
    "            pass\n",
    "        \n",
    "        return self.iterations\n",
    "    \n",
    "    def plot(self):\n",
    "        ''' plot hypothesis function and color coded data points '''\n",
    "        # separate data into positive and negative, scatter plot them by color\n",
    "        positive = [p for i,p in enumerate(self.x) if self.y[i] == 1]\n",
    "        negative = [p for i,p in enumerate(self.x) if self.y[i] == -1]\n",
    "\n",
    "        for p in positive:\n",
    "            plt.scatter(p[1], p[2], s = 50,c='blue')\n",
    "\n",
    "        for n in negative:\n",
    "            plt.scatter(n[1], n[2], s = 50, c='red')\n",
    "\n",
    "        \n",
    "        # decision boundary line (solve weights equation x_0 *w_0 + x_1 * w_1 + x_2 * w_2 = 0)\n",
    "        x = np.arange(0,1,0.01)\n",
    "        hyp = lambda x: (-test.w[1] / test.w[2]) * x - (test.w[0] / test.w[2])\n",
    "\n",
    "        plt.plot(x, hyp(x))\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trial:\n",
    "    ''' class for conducting trials of Perceptron Model '''\n",
    "    def __init__(self, N, trials = 1000):\n",
    "        # number of perceptrons\n",
    "        self.N = N\n",
    "        \n",
    "        # number of trials (defaults to 1000)\n",
    "        self.trials = trials\n",
    "        \n",
    "    def run(self):\n",
    "        # list for holding # of iterations and probabilities\n",
    "        iterations = []\n",
    "        probabilities = []\n",
    "        for i in range(self.trials):\n",
    "            # run trial, get iterations for convergence\n",
    "            trial = Perceptron(self.N)\n",
    "            iterations.append(trial.run())\n",
    "            \n",
    "            probabilities.append(self.prob(trial))\n",
    "            \n",
    "        average_it = sum(iterations) / len(iterations)\n",
    "        average_prob = sum(probabilities) / len(probabilities)\n",
    "        print(\"Took {} iterations on average for {} perceptrons.\".format(average_it, self.N))\n",
    "        \n",
    "        print(\"Average probability that f(x) not equal to g(x) is {}.\".format(average_prob))\n",
    "\n",
    "    def prob(self, trial):\n",
    "        ''' generates new data and approximates out of sample error of trial '''\n",
    "        test_data = np.random.uniform(0, 1, (1000, 2))\n",
    "        \n",
    "        # get proper classification of test data\n",
    "        test_y = trial.classify(test_data)\n",
    "        \n",
    "        # add 1s to test_data to account for weight threshhold\n",
    "        test_data = np.concatenate((np.repeat(1,1000).reshape(1000, 1), test_data), axis=1)\n",
    "        \n",
    "        # use hypothesis to estimate classification\n",
    "        train_y = trial.h(test_data)\n",
    "        \n",
    "        # error is points of non convergence\n",
    "        error = test_y[test_y != train_y]\n",
    "        \n",
    "        return len(error) / len(test_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 20.218 iterations on average for 10 perceptrons.\n",
      "Average probability that f(x) not equal to g(x) is 0.11232200000000007.\n"
     ]
    }
   ],
   "source": [
    "T = Trial(10)\n",
    "T.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Take N = 10. How many iterations does it take on average for the PLA to\n",
    "converge for N = 10 training points? Pick the value closest to your results\n",
    "(again, ‘closest’ means: |your answer − given option| is closest to 0).\n",
    "\n",
    "A. From above, it took roughly 20 iterations, which is closest to \n",
    "\n",
    "__[b] 15 __\n",
    "\n",
    "Q8. Which of the following is closest to P[f(x) 6= g(x)] for N = 10?\n",
    "\n",
    "A. Again from above, closest answer is \n",
    "\n",
    "__[c] 0.1 __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 161.029 iterations on average for 100 perceptrons.\n",
      "Average probability that f(x) not equal to g(x) is 0.01285399999999996.\n"
     ]
    }
   ],
   "source": [
    "T = Trial(100)\n",
    "T.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Now, try N = 100. How many iterations does it take on average for the PLA\n",
    "to converge for N = 100 training points? Pick the value closest to your results.\n",
    "\n",
    "Took roughly 150 iterations on average, which is closest to, \n",
    "\n",
    "__[b] 100__\n",
    "\n",
    "Q10.  Which of the following is closest to P[f(x) 6= g(x)] for N = 100?\n",
    "\n",
    "From above, closest to, \n",
    "\n",
    "__[b] 0.01 __"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
