{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. For an $\\mathcal{H}$ with $d_{VC} = 10$, if you want 95% confidence that your generalization error is at most 0.05, what is the closest numerical approximation of the sample size that the VC generalization bound predicts?\n",
    "\n",
    "A. \n",
    "\n",
    "By using the generalization bound, and approximating $m_{\\mathcal{H}}(2N) \\approx (2N)^{d_{VC}}$, we get that\n",
    "\n",
    "\n",
    "$$N \\geq \\frac{8}{\\epsilon^2} \\log \\left ( \\frac{4((2N)^{d_{VC}} + 1)}{\\delta} \\right )$$, \n",
    "\n",
    "so when $d_{VC} = 10$, $\\epsilon = 0.05$ and $\\delta = 0.05$, we get\n",
    "\n",
    "$$N \\geq \\frac{8}{0.05^2} \\log \\left ( \\frac{4(2N)^{10} + 1)}{0.05} \\right )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def estimateN(N, epsilon, delta, dvc):\n",
    "    ''' estimation function for determining examples given initial estimate N'''\n",
    "    estimate = (8 / (epsilon*epsilon)) * math.log((4* ((2*N)**10) + 1) / delta)\n",
    "    \n",
    "    if (estimate - N)**2 <= 0.01:\n",
    "        return estimate\n",
    "    else:\n",
    "        return estimateN(estimate, epsilon, delta, dvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452956.8624225618"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimateN(1000, 0.05, 0.05, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ A1. [d] 460,000 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. There are a number of bounds on the generalization error $\\epsilon$, all holding with probability at least $1 - \\delta$. Fix $d_{VC} = 50$ and $\\delta = 0.05$ and plot these bounds as a function of $N$. Which bound is the smallest for very large N, say N = 10, 000? Note that [c] and [d] are implicit bounds in $\\epsilon$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For N = 10000:\n",
      "vc is 0.632174915200836\n",
      "Rademacher is 0.3313087859616395\n",
      "Parrondo and Van den Broek is 0.22382177493925573\n",
      "Devroye is 0.2152281052374131\n",
      "\n",
      "For N = 5:\n",
      "vc is 13.828161484991483\n",
      "Rademacher is 7.048776564183685\n",
      "Parrondo and Van den Broek is 5.101361964962139\n",
      "Devroye is 5.5979442138337605\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJkiCuxBCNEES3IUQogn6fyQzWRwv53GkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d056390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "dvc = 50\n",
    "delta = 0.05\n",
    "def vc(N):\n",
    "    ''' original VC bound: epsilon as function of N'''\n",
    "    return math.sqrt( (8/N) * math.log((4* (2*N)**dvc) / delta))\n",
    "    \n",
    "def radPen(N):\n",
    "    ''' Rademacher Penalty Bound '''\n",
    "    return math.sqrt((2 * math.log(2*N * N**dvc))/ N) + math.sqrt(2/N * math.log(1/delta)) + 1 / N\n",
    "    \n",
    "def parVan(N):\n",
    "    '''Parrondo and Van den Broek Bound: implicit formula estimated with initial guess of 1'''\n",
    "    def estParVan(N, eps):\n",
    "        estimate = math.sqrt(1/N * (2 * eps + math.log( (6 * (2*N)**dvc) / delta )))\n",
    "        if (estimate - eps) <= 0.00001:\n",
    "            return estimate\n",
    "        else:\n",
    "            return estParVan(N, estimate)\n",
    "    return estParVan(N, 0.5)\n",
    "    \n",
    "    # use other bound as initial guess\n",
    "    guess = radPen(N)\n",
    "    \n",
    "    return estParVan(N, guess)\n",
    "    \n",
    "def devroye(N):\n",
    "    ''' Devroye Bound: implicit formula estimated with initial guess of 1'''\n",
    "    def estDev(N, eps):\n",
    "        estimate = math.sqrt(1/(2*N) * ( 4*eps*(1+eps) +  math.log(4 / delta) + dvc * math.log(N**2) ) )\n",
    "        if (estimate - eps)**2 <= 0.0001:\n",
    "            return estimate\n",
    "        else:\n",
    "            return estDev(N, estimate)\n",
    "        \n",
    "    # use other bound as initial guess\n",
    "    guess = radPen(N)\n",
    "    return estDev(N, guess)\n",
    "\n",
    "N = np.arange(1, 10000, 5)\n",
    "\n",
    "plt.plot(N, np.vectorize(vc)(N), label = \"VC Bound\")\n",
    "plt.plot(N, np.vectorize(radPen)(N), label = \"Rademacher Penalty Bound\")\n",
    "plt.plot(N, np.vectorize(parVan)(N), label = \"Parrando and Van den Broek Bound\")\n",
    "#plt.plot(N, np.vectorize(devroye)(N), label = \"Devroye Bound\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 5)\n",
    "plt.xlim(10, 10000)\n",
    "\n",
    "def printTestValues(test_N):\n",
    "    print(\"For N = {}:\".format(test_N))\n",
    "    print(\"vc is {}\".format(vc(test_N)))\n",
    "    print(\"Rademacher is {}\".format(radPen(test_N)))\n",
    "    print(\"Parrondo and Van den Broek is {}\".format(parVan(test_N)))\n",
    "    print(\"Devroye is {}\".format(devroye(test_N)))\n",
    "\n",
    "# Q2\n",
    "printTestValues(10000)\n",
    "print()\n",
    "#Q3\n",
    "printTestValues(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2. [d] __\n",
    "\n",
    "__A3. [c] __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where the target function $f : [−1, 1] \\rightarrow \\mathbb{R}$ is given by $f(x) = \\sin(\\pi x)$ and the input probability distribution is uniform on $[−1, 1]$. Assume that the training set has only two examples (picked independently), and that the learning algorithm produces the hypothesis that minimizes the mean squared error on the examples.\n",
    "\n",
    "Q4. Assume the learning model consists of all hypotheses of the form $h(x) = ax$.\n",
    "What is the expected value, $\\bar{g}(x)$, of the hypothesis produced by the learning\n",
    "algorithm (expected value with respect to the data set)? Express your $\\bar{g}(x)$ as\n",
    "$\\bar{a}x$ , and round $\\bar{a}$ to two decimal digits only, then match exactly to one of the\n",
    "following answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A4. With just two training examples, $(x_1, y_1)$ and $(x_2, y_2)$, and $h(x) = ax$, the average mean squared error is, \n",
    "\n",
    "$$\\epsilon(a) = \\frac{1}{2} \\left [ (a x_1 - y_1)^2 + (a x_2 - y_2)^2 \\right ] $$\n",
    "\n",
    "where, setting the derivative to $0$ to find minimizing value of $a$, \n",
    "\n",
    "$$\\frac{d \\epsilon}{d a} = x_1(a x_1 - y_1) + x_2(ax_2 - y_2) = ax_1^2 + ax_2^2 - x_1y_1 - x_2y_2 = 0 $$\n",
    "\n",
    "$$a(x_1^2 + x_2^2) = x_1y_1 + x_2y_2$$\n",
    "\n",
    "$$ a = \\frac{x_1y_1 + x_2y_2}{x_1^2 + x_2^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43\n"
     ]
    }
   ],
   "source": [
    "def run(data = None):\n",
    "    ''' run learning algorithm once, return hypothesis'''\n",
    "    # 2 training examples\n",
    "    train = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "    # target function\n",
    "    f = lambda x: np.sin(np.pi * x)\n",
    "\n",
    "    # evaluate training examples with target function\n",
    "    y = f(train)\n",
    "\n",
    "    return (np.inner(train, y) / np.sum(np.square(train)))\n",
    "    \n",
    "\n",
    "# run 1000 times\n",
    "gs = [run() for n in range(10000)]\n",
    "\n",
    "# average is expected g; truncated to 2 decimal points\n",
    "a_avg = sum(gs) / len(gs)\n",
    "print(\"{0:.2f}\".format(a_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 4A. [e] None of the above __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27074272572060754"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternate monte carlo estimate for bias\n",
    "t1 = np.random.uniform(-1, 1, 100000)\n",
    "\n",
    "f = lambda x: np.sin(np.pi * x)\n",
    "\n",
    "np.average(np.square(1.43 *t1 - f(t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bias,\n",
    "\n",
    "$$\\text{bias} = \\mathbb{E}_x [\\text{bias}(x) ] = \\mathbb{E}_x [(\\bar{g}(x) - f(x))^2] $$\n",
    "\n",
    "\n",
    "So with $\\bar{g}(x) = 1.43x$ and $f(x) = \\sin(\\pi x)$, \n",
    "\n",
    "$$\\text{bias} = \\mathbb{E}_X [ (1.43x - \\sin(\\pi x) )^2 ] $$\n",
    "\n",
    "$$ \\text{bias} = \\int_{x=-1}^{1} p(x) (1.43x - \\sin(\\pi x))^2 $$\n",
    "\n",
    "Where $p(x)$ is a _uniform_ probability from $-1$ to $1$, i.e., \n",
    "\n",
    "$$p(x) = \\frac{1}{1 - (-1)} = \\frac{1}{2}$$, \n",
    "\n",
    "so, \n",
    "\n",
    "$$\\text{bias} = \\frac{1}{2} \\int_{x=-1}^{1} (1.43 x - \\sin(\\pi x))^2 \\approx 0.271$$\n",
    "\n",
    "\n",
    "__ 5A. [b] 0.3 __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is the closest value to the variance in this case? \n",
    "\n",
    "A. We estimate it by randomly generating samples of $\\mathcal{D}$, evaluating the expected value (and noting again that $p(x)$ is uniform, so equals $1/2$), and then averaging these expected values to estimate the variance. \n",
    "\n",
    "$$\\text{var} = \\mathbb{E}_{X} [ \\mathbb{E}_{\\mathcal{D}} [ (g^{(\\mathcal{D})}(x) - \\bar{g}(x))^2 ]] $$\n",
    "\n",
    "We approximate this with a simple Monte Carlo method as follows: randomly choose two points that make up a given data set $\\mathcal{D}$. Use these two points to determine a hypothesis $g^{(\\mathcal{D})}$, and then get a new set of test points as $x$ between -1 and 1 to approximate the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23199243506192993"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.random.uniform(-1, 1, 10000)\n",
    "f = lambda x: np.sin(np.pi * x)\n",
    "\n",
    "var = np.empty(1000)\n",
    "for i in range(1000):\n",
    "    g = run()\n",
    "    test = np.random.uniform(-1, 1, 1000)\n",
    "    # multiply each estimated variance by 0.5 since there are d\n",
    "    var[i] = np.average(np.square(g*test - 1.43*test))\n",
    "\n",
    "\n",
    "np.average(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6. [a] __ 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Now, let’s change $\\mathcal{H}$. Which of the following learning models has the least expected value of out-of-sample error?\n",
    "\n",
    "For the following, we calculate the bias and variance and sum them since,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We express the error of each of the hypothesis as being transformations of the squared error of linear regression. Explicitly, let $\\theta = [b, a]$, so that the minimizing value is given by,\n",
    "\n",
    "$$\\mathbf{\\theta} = (X^T X)^{-1} X^T \\mathbf{y}$$\n",
    "\n",
    "For $h(x) = b$, let $x_i = [1, 0]$ \n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h(x) = ax &\\implies x_i = [x, 0] \\\\\n",
    "h(x) = ax + b &\\implies x_i = [x, 1] \\\\\n",
    "h(x) = ax^2  &\\implies x_i = [x^2, 0] \\\\\n",
    "h(x) = ax^2 + b &\\implies x_i = [x^2, 1]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to transform and combine\n",
    "combine = lambda a,b: np.concatenate((a.reshape(1, a.shape[0]), b.reshape(1, b.shape[0])))\n",
    "\n",
    "# target function\n",
    "f = lambda x: np.sin(np.pi * x)\n",
    "\n",
    "# transformed xs\n",
    "x_a = lambda x: combine(np.zeros(x.shape[0]), np.zeros(x.shape[0]))\n",
    "x_b = lambda x: combine(x, np.zeros(x.shape[0]))\n",
    "x_c = lambda x: combine(x, np.ones(x.shape[0]))\n",
    "x_d = lambda x: combine(np.square(x), np.zeros(x.shape[0]))\n",
    "x_e = lambda x: combine(np.square(x), np.ones(x.shape[0]))\n",
    "xs = [x_a, x_b, x_c, x_d, x_e]\n",
    "\n",
    "theta = lambda x,y: np.inner(np.linalg.pinv(x).T, y) \n",
    "\n",
    "# data to approximate expected g value\n",
    "data = np.random.uniform(-1, 1, 1000)\n",
    "\n",
    "# thetas for all values\n",
    "thetas = [theta(x(data), f(data)) for x in xs]\n",
    "\n",
    "\n",
    "def g(x, theta):\n",
    "    ''' returns hypotheses given theta '''\n",
    "    return x * theta[0] + theta[1]\n",
    "\n",
    "def bias(theta, f):\n",
    "    ''' estimates bias given hypothesis (theta) and target function '''\n",
    "    test = np.random.uniform(-1, 1, 1000)\n",
    "    return np.average(np.square(g(test, theta) - f(test)))\n",
    "\n",
    "# get variances\n",
    "variances = []\n",
    "\n",
    "for j, t in enumerate(thetas):\n",
    "    variances.append(np.empty(1000))\n",
    "    for i in range(1000):\n",
    "        # smaller training data of just 2 examples\n",
    "        train = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "        \n",
    "        g_t = theta(xs[j](train), f(train))\n",
    "\n",
    "        # test data to compare with average hypothesis\n",
    "        test = np.random.uniform(-1, 1, 1000)\n",
    "\n",
    "        # get average hypothesis for test\n",
    "        variances[j][i] = np.average(np.square(g(test, g_t) - g(test, t)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49696437478\n",
      "0.532258606647\n",
      "1.83423175771\n",
      "24.6238258668\n",
      "3447.68076466\n"
     ]
    }
   ],
   "source": [
    "exp_biases = [bias(t, f) for t in thetas]\n",
    "\n",
    "exp_var = [np.average(v) for v in variances]\n",
    "\n",
    "\n",
    "error = (bis + vas for bis,vas in zip(exp_biases, exp_var))\n",
    "\n",
    "for err in error:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ A7. [a] Hypotheses of the form h(x) = b __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Assume $q \\geq 1$ is an integer and let $m_{\\mathcal{H}}(1) = 2$. What is the VC dimension of a hypothesis set whose growth function satisfies: $m_{\\mathcal{H}}(N + 1) = 2 m_{\\mathcal{H}}(N) − {N \\choose q}$ ? Recall that ${M \\choose m} = 0$ when $m > M$. \n",
    "\n",
    "Note that for any $N < q$, ${N \\choose q} = 0$, so that, \n",
    "\n",
    "$$m_{\\mathcal{H}}(2) = 2 m_{\\mathcal{H}}(1) = 2^2 $$\n",
    "$$m_{\\mathcal{H}}(3) = 2 m_{\\mathcal{H}}(2) = 2^3 $$\n",
    "$$m_{\\mathcal{H}}(q) = 2m_{\\mathcal{H}}(q-1) = 2 (2 m_{\\mathcal{H}}(q-2)) = 2 ( 2( \\ldots (2 m_{\\mathcal{H}}(1))) = 2^q$$\n",
    "\n",
    "Then, since, \n",
    "\n",
    "$$m_{\\mathcal{H}}(q+1) = 2 m_{\\mathcal{H}}(q) - {q \\choose q} = 2^{q+1} - 1 < 2^{q+1}$$\n",
    "\n",
    "It follows that the breakpoint is $q$. \n",
    "\n",
    "__ 8A. [c] __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. For hypothesis sets $\\mathcal{H}_1, \\mathcal{H}_2, \\ldots, \\mathcal{H}_K$ with finite, positive VC dimensions $d_{vc}(\\mathcal{H}_k)$, some of the following bounds are correct and some are not. Which among the correct ones is the tightest bound (the smallest range of values) on the VC dimension of the __intersection__ of the sets: $d_{vc}(\\cap_{k=1}^{K} \\mathcal{H}_k)$? (The VC dimension of an empty set or a singleton set is taken as zero)\n",
    "\n",
    "We start from the tightest possible lower bound and check its correctness, i.e.,  \n",
    "\n",
    "$$ \\text{min} \\{ d_{VC} (\\mathcal{H}_K) \\}^K_{k=1} \\leq d_{VC} (\\cap_{k=1}^K \\mathcal{H}_k) $$\n",
    "\n",
    "To disprove the left side, consider that if $(\\cap_{k=1}^K \\mathcal{H}_k) = \\emptyset$, then $d_{VC} (\\mathcal{H}_k) = 0$. Since each hypothesis set has positive VC dimension by construction, the min of the set is strictly larger than $0$, so the next tightest possible bound is now, \n",
    "\n",
    "$$0 \\leq d_{VC} (\\cap_{k=1}^K \\mathcal{H}_k) $$\n",
    "\n",
    "For the righthand side, the tightest possible bound is \n",
    "\n",
    "$$ d_{VC} (\\cap_{k=1}^K \\mathcal{H}_k)  \\leq \\text{min} \\{d_{VC} (\\mathcal{H}_k) \\}_{k=1}^{K} $$\n",
    "\n",
    "Clearly the subset of a hypothesis set can shatter at _most_ as many points as its superset. Since the intersection is a subset of each hypothesis set, it follows it's also a subset of the hypothesis set with the minimum VC dimension and the above holds. \n",
    "\n",
    "__9A. [b] $0 \\leq d_{VC} (\\cap_{k=1}^{K} \\mathcal{H}_k ) \\leq d_{VC} (\\mathcal{H}_i)$ __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. For hypothesis sets $\\mathcal{H}_1, \\mathcal{H}_2, \\ldots, \\mathcal{H}_K$ with finite, positive VC dimensions $d_{vc}(\\mathcal{H}_k)$, some of the following bounds are correct and some are not. Which among the correct ones is the tightest bound (the smallest range of values) on the VC dimension of the __union__ of the sets: $d_{vc}(\\cup_{k=1}^{K} \\mathcal{H}_k)$? \n",
    "\n",
    "Similarly, we start from the tightest possible lower bound and check its correctness: \n",
    "\n",
    "$$\\text{max} \\{d_{VC} (\\mathcal{H}_k) \\}_{k=1}^{K} \\leq d_{vc}(\\cup_{k=1}^{K} \\mathcal{H}_k)$$\n",
    "\n",
    "We use the same argument as above (that a subset's VC dimension is at most that of its superset), noting that every hypothesis set is a subset of the union, so that the hypothesis set with the max VC dimension is also a subset of the union and thus the above holds. \n",
    "\n",
    "The tightest possible upper bound is, \n",
    "\n",
    "$$ d_{VC}(\\cup_{k=1}^{K} \\mathcal{H}_k) \\leq \\sum_{k=1}^K d_{VC} (\\mathcal{H}_k) $$\n",
    "\n",
    "For a counterexample, consider the case of two hypothesis sets $\\mathcal{H_1}$ and $\\mathcal{H_2}$ on $\\mathbb{R}^2$. Suppose $\\mathcal{H_1}$ can label just one point as $+1$ or $-1$ (so that all the rest become the opposite sign) and that $\\mathcal{H_2}$ can only label either all points as $+1$ or all points as $-1$. Clearly each hypothesis set can only shatter one point. Their union, however, can shatter 3 points. So the only other possibility is \n",
    "\n",
    "$$ d_{VC}(\\cup_{k=1}^{K} \\mathcal{H}_k) \\leq K - 1 + \\sum_{k=1}^K d_{VC} (\\mathcal{H}_k) $$\n",
    "\n",
    "__10A. [e] $\\text{max} \\{d_{VC} (\\mathcal{H}_k) \\}_{k=1}^{K} \\leq d_{vc}(\\cup_{k=1}^{K} \\mathcal{H}_k)  \\leq K - 1 + \\sum_{k=1}^K d_{VC} (\\mathcal{H}_k)$ __"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
